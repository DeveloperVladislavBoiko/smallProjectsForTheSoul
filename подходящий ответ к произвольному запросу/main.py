import re
import string
import numpy as np
from typing import List, Dict, Optional, Set
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pymorphy3
from functools import lru_cache


class AdvancedRussianCommandMatcher:
    def __init__(self, commands_file="commands.txt"):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        self.morph = pymorphy3.MorphAnalyzer()

        self.commands = self._load_commands(commands_file)
        self.synonyms = self._load_synonyms()
        self.stop_words = self._get_stop_words()

        # –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å—ã
        self._build_indices()

    def _load_commands(self, file_path: str) -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–º–∞–Ω–¥ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                commands = [line.strip() for line in f if line.strip()]
                if not commands:
                    raise ValueError(f"–§–∞–π–ª '{file_path}' –ø—É—Å—Ç")
                return commands
        except FileNotFoundError:
            raise FileNotFoundError(f"–§–∞–π–ª —Å –∫–æ–º–∞–Ω–¥–∞–º–∏ '{file_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω")

    def _load_synonyms(self) -> Dict[str, Set[str]]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º —Å–ª–æ–≤"""
        synonyms = {
            '–ø–æ–≥–æ–¥–∞': {'–ø—Ä–æ–≥–Ω–æ–∑', '–º–µ—Ç–µ–æ', '–¥–æ–∂–¥—å', '—Å–æ–ª–Ω—Ü–µ', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞'},
            '–≤–∫–ª—é—á–∏—Ç—å': {'–∑–∞–ø—É—Å—Ç–∏—Ç—å', '–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å', '–≤—Ä—É–±–∏—Ç—å', '—Å—Ç–∞—Ä—Ç–æ–≤–∞—Ç—å'},
            '–≤—ã–∫–ª—é—á–∏—Ç—å': {'–æ—Ç–∫–ª—é—á–∏—Ç—å', '–¥–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å', '—Å—Ç–æ–ø', '–æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å'},
            '–º—É–∑—ã–∫–∞': {'–ø–µ—Å–Ω—è', '—Ç—Ä–µ–∫', '–º–µ–ª–æ–¥–∏—è', '–∞—É–¥–∏–æ', '–∑–≤—É–∫'},
            '–Ω–∞–π—Ç–∏': {'–ø–æ–∏—Å–∫', '–æ—Ç—ã—Å–∫–∞—Ç—å', '—Ä–∞–∑—ã—Å–∫–∞—Ç—å', '–æ–±–Ω–∞—Ä—É–∂–∏—Ç—å'},
            '–æ—Ç–∫—Ä—ã—Ç—å': {'–∑–∞–ø—É—Å—Ç–∏—Ç—å', '—Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å', '–≤–∫–ª—é—á–∏—Ç—å', '–ø–æ–∫–∞–∑–∞—Ç—å'},
            '–∑–∞–∫—Ä—ã—Ç—å': {'–≤—ã–∫–ª—é—á–∏—Ç—å', '—Å–≤–µ—Ä–Ω—É—Ç—å', '–æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å', '–∑–∞–≤–µ—Ä—à–∏—Ç—å'},
            '—Å–∫–∞–∑–∞—Ç—å': {'–ø—Ä–æ–∏–∑–Ω–µ—Å—Ç–∏', '–æ–∑–≤—É—á–∏—Ç—å', '–ø–æ–∫–∞–∑–∞—Ç—å', '—Å–æ–æ–±—â–∏—Ç—å'},
            '–ø–æ–∫–∞–∑–∞—Ç—å': {'–æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å', '–≤—ã–≤–µ—Å—Ç–∏', '–ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å', '–æ—Ç–∫—Ä—ã—Ç—å'}
        }

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–Ω–æ–Ω–∏–º–∞
        expanded_synonyms = {}
        for word, syns in synonyms.items():
            expanded_set = set(syns)
            expanded_set.add(word)
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã
            for s in list(expanded_set):
                expanded_set.add(self._get_normal_form(s))
            expanded_synonyms[word] = expanded_set

        return expanded_synonyms

    def _get_stop_words(self) -> Set[str]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
        return {
            '—ç—Ç–æ', '–∫–∞–∫', '—Ç–∞–∫', '–∏', '–≤', '–Ω–∞–¥', '–∫', '–¥–æ', '–Ω–µ', '–Ω–∞', '–Ω–æ', '–∑–∞', '—Ç–æ', '—Å', '–ª–∏',
            '–∞', '–≤–æ', '–æ—Ç', '—Å–æ', '–¥–ª—è', '–æ', '–∂–µ', '–Ω—É', '–≤—ã', '–±—ã', '—á—Ç–æ', '–∫—Ç–æ', '–æ–Ω', '–æ–Ω–∞', '–º–Ω–µ',
            '–º–Ω–æ–π', '–º–µ–Ω—è', '—Ç–µ–±–µ', '—Ç–æ–±–æ–π', '—Ç–µ–±—è', '–Ω–∞–º', '–Ω–∞–º–∏', '–Ω–∞—Å', '–≤–∞–º', '–≤–∞–º–∏', '–≤–∞—Å', '–∏—Ö',
            '–∏–º–∏', '–µ–≥–æ', '–µ—ë', '–∏–º', '–Ω–∏–º–∏', '–Ω–µ–≥–æ', '–Ω–µ—ë', '–Ω–∏—Ö', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–µ'
        }

    @lru_cache(maxsize=10000)
    def _get_normal_form(self, word: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        parsed = self.morph.parse(word)[0]
        return parsed.normal_form

    def _preprocess_text(self, text: str) -> str:
        """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏"""
        text = text.lower().strip()
        text = re.sub(r'[{}]'.format(string.punctuation), ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text

    def _tokenize(self, text: str) -> List[str]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–∏–Ω–æ–Ω–∏–º–æ–≤"""
        text = self._preprocess_text(text)
        words = text.split()

        processed_words = []
        for word in words:
            if word in self.stop_words:
                continue

            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É —Å–ª–æ–≤–∞
            normal_form = self._get_normal_form(word)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
            found_synonyms = set()
            for base_word, syns in self.synonyms.items():
                if normal_form in syns or word in syns:
                    found_synonyms.add(base_word)

            if found_synonyms:
                processed_words.extend(found_synonyms)
            else:
                processed_words.append(normal_form)

        return processed_words

    def _build_indices(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        # –°–æ–∑–¥–∞–µ–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É
        self.vectorizer = TfidfVectorizer(tokenizer=self._tokenize)
        command_texts = [" ".join(self._tokenize(cmd)) for cmd in self.commands]
        self.tfidf_matrix = self.vectorizer.fit_transform(command_texts)

        # –ò–Ω–¥–µ–∫—Å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–æ–º–∞–Ω–¥—ã
        self.command_keywords = []
        for cmd in self.commands:
            keywords = set(self._tokenize(cmd))
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
            expanded_keywords = set()
            for word in keywords:
                expanded_keywords.add(word)
                for base, syns in self.synonyms.items():
                    if word in syns:
                        expanded_keywords.add(base)
            self.command_keywords.append(expanded_keywords)

    def _get_semantic_similarity(self, query: str, command: str, command_idx: int) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å —É—á–µ—Ç–æ–º —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏"""
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∑–∞–ø—Ä–æ—Å –≤ –Ω–∞–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤
        query_tokens = set(self._tokenize(query))
        command_tokens = self.command_keywords[command_idx]

        # –ü—Ä–æ—Å—Ç–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
        common_tokens = query_tokens & command_tokens
        token_sim = len(common_tokens) / max(len(query_tokens),
                                             len(command_tokens)) if query_tokens and command_tokens else 0

        # TF-IDF –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        query_vec = self.vectorizer.transform([" ".join(query_tokens)])
        cmd_vec = self.vectorizer.transform([" ".join(command_tokens)])
        tfidf_sim = cosine_similarity(query_vec, cmd_vec)[0][0]

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        return 0.7 * token_sim + 0.3 * tfidf_sim

    def find_best_match(self, query: str, threshold: float = 0.65) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –Ω–∞–∏–ª—É—á—à–µ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Å —É—á–µ—Ç–æ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏ –∏ —Å–∏–Ω–æ–Ω–∏–º–æ–≤"""
        if not query or not self.commands:
            return None

        best_match = None
        best_score = 0.0

        for idx, cmd in enumerate(self.commands):
            score = self._get_semantic_similarity(query, cmd, idx)

            if score > best_score and score >= threshold:
                best_score = score
                best_match = cmd

        # –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ—Ç, –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —á–∞—Å—Ç–∏—á–Ω—ã–µ
        if best_match is None:
            for idx, cmd in enumerate(self.commands):
                score = self._get_semantic_similarity(query, cmd, idx)
                if score > 0.4 and (best_match is None or score > best_score):
                    best_score = score
                    best_match = cmd

            if best_match:
                return f"–í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É: {best_match}"

        return best_match


def main():
    try:
        print("üîç –£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥ (—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫)")
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞...")

        matcher = AdvancedRussianCommandMatcher()

        print("–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ. –î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ '–≤—ã—Ö–æ–¥'")
        while True:
            query = input("\n–í–∞—à –∑–∞–ø—Ä–æ—Å: ").strip()
            if query.lower() in ('–≤—ã—Ö–æ–¥', 'exit', 'quit'):
                break

            best_match = matcher.find_best_match(query)

            if best_match:
                print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç: {best_match}")
            else:
                print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –∫–æ–º–∞–Ω–¥—É")
                print("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏")

    except Exception as e:
        print(f"\n‚ö† –û—à–∏–±–∫–∞: {e}")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ commands.txt –∏ —É—Å—Ç–∞–Ω–æ–≤–∫—É pymorphy3")


if __name__ == "__main__":
    # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ pymorphy3 –µ—Å–ª–∏ –µ—â–µ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: pip install pymorphy3
    main()
  
